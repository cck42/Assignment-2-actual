---
title: "Solutions to problems 1, 4, and 5 on homework 2"
author: "cck42"
date: "10/23/2019"
output: html_document
vignette: >
  %\VignetteIndexEntry{"Assignment 2 solutions"}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(casl)
library(MASS)
```

##### Question 1: CASL 2.11 problem 5  
The general form for a linear model is:  
$$
\hat \beta = (X^t * X)^{-1} X^T y
$$
where X is the set of predictor values and y is the response values.  

In the case of a scalar X and an intercept:
$$
X = 
\begin{matrix} 
x_1 & 1 \\
x_2 & 1 \\
... & ... \\
x_p & 1
\end{matrix}
$$
and:  
$$
X^T =
\begin{pmatrix} 
x_1 & x_2 & ... & x_p \\
1 & 1 & ... & 1 
\end{pmatrix} \\ 
$$
$$
X^T * X = 
\begin{pmatrix} 
x_1 & 1 \\
x_2 & 1 \\
... & ... \\
x_p & 1
\end{pmatrix}
\begin{pmatrix} 
x_1 & x_2 & ... & x_p \\
1 & 1 & ... & 1
\end{pmatrix} \\
= \begin{pmatrix}
\sum_{n=1}^p x_n^2 & \sum_{n=1}^p x_n \\
\sum_{n=1}^p x_n & 1
\end{pmatrix}
$$
The inverse of this is:
$$
\begin{pmatrix}
1 & -\sum_{n=1}^p x_n \\
-\sum_{n=1}^p x_n & \sum_{n=1}^p x_n^2
\end{pmatrix}
$$
$X^T * y$ is:
$$
X^T * y = X^T = 
\begin{pmatrix} 
x_1 & x_2 & ... & x_p \\
1 & 1 & ... & 1
\end{pmatrix} *
\begin{pmatrix}
y_1 \\
y_2 \\
... \\
y_p
\end{pmatrix}\\
=
\begin{pmatrix}
\sum_{n=1}^p x_n y_n \\
\sum_{n=1}^p y_n
\end{pmatrix}
$$
Putting these together, we get:
$$
\begin{pmatrix}
1 & -\sum_{n=1}^p x_n \\
-\sum_{n=1}^p x_n & \sum_{n=1}^p x_n^2
\end{pmatrix} * \begin{pmatrix}
\sum_{n=1}^p x_n y_n \\
\sum_{n=1}^p y_n
\end{pmatrix}n\\
$$
$$
= \begin{pmatrix}
\sum_{n=1}^p x_ny_n - (\sum_{n=1}^p x_n*\sum_{n=1}^p x_ny_n) \\
-\sum_{n=1}^p x_n * \sum_{n=1}^p y_n + (\sum_{n=1}^p x_n^2 * \sum_{n=1}^p y_n)
\end{pmatrix}
$$
Which simplifies to:
$$
\begin{pmatrix}
\sum_{n=1}^p x_ny_n * (1-\sum_{n=1}^p x_n) \\
\sum_{n=1}^p y_n * (\sum_{n=1}^p x_n^2 - \sum_{n=1}^p x_n)
\end{pmatrix}
$$
Since this is equal to $\hat \beta$, and $\hat \beta = \begin{pmatrix} \hat \beta_0 \\ \hat \beta_1 \end{pmatrix}$:
$$
\hat \beta = 
\begin{pmatrix} \hat \beta_0 \\
\hat \ beta_1 \end{pmatrix}
 = \begin{pmatrix}
\sum_{n=1}^p x_ny_n * (1-\sum_{n=1}^p x_n) \\
\sum_{n=1}^p y_n * (\sum_{n=1}^p x_n^2 - \sum_{n=1}^p x_n)
\end{pmatrix}
$$
Our estimators are:
$$
\beta_0 = \sum_{n=1}^p x_ny_n * (1-\sum_{n=1}^p x_n) \\
\beta_1 = \sum_{n=1}^p y_n * (\sum_{n=1}^p x_n^2 - \sum_{n=1}^p x_n)
$$

***  

##### Question 4: Numerical Stability and Statistical Error  

>Section 2.8 of CASL shows that as the numerical stability decreases, statistical errors increase. Reproduce
the results and then show that using ridge regression can increase numerical stability and decrease
statistical error.  


**Intuitive explanation:**  
From equations 2.48 through 2.54, there were a few key points:  
+ The penalty in estimating v from AV (e.g. relative error between the two) is determined by $\frac{c_{max}}{c_{min}}$  
+ If the matrix in question (A) does not have full column rank, $c_min$ goes to zero (colinear or fewer observations than columns)  
+ If $c_min$ goes to zero, the penalty goes to infinity; the error in estimating v has an infinitely high (or at least very high) limit  

But this is the theoretical limit to the computational error; it isn't necessarily that high. So in what cases do we approach this limit and not? The statistical error vector in any linear system becomes part of this error calculation; though it is possible to get the numerical error reduced to within the precision of whatever machine is being used, statistical error can't be changed. The statistical error is also multiplied by the same conversion factor, $\frac{c_{max}}{c_{min}}$. In this way, the numerical instability becomes significant.  

**Example in R**  
*(pretty much all of this is right out of CASL, with my own explanations)*  
First, I'll make a matrix with some samples randomly sampled from the normal distribution and a dummy regression vector, beta:
```{r}
n <- 1000; p <- 25
beta <- c(1,rep(0,p-1))
X <- matrix(rnorm(n*p),ncol=p)

svals <- svd(X)$d
cond <- max(svals)/min(svals)
print(c("The condition number for X is: ",cond))
```
So, this condition isn't crazy high. Let's calculate the MSE:
```{r}
N<-1e4
errors <- rep(0,N)
for(k in 1:N){
  y <- X %*% beta + rnorm(n)
  betahat <- casl_ols_svd(X,y)
  errors[k] <- sqrt(sum((betahat-beta)^2))
}
avg_error <- mean(errors)
print(c("The mean squared error for X is: ",avg_error))
```
This MSE is acceptable! Now, I'm going to do the same thing, but instead of randomly sampling from the normal distribution, I'm going to take this exact matrix and make the first column a product of the original first and second columns. This means that the first two columns are highly colinear.
```{r}
alpha <- 0.001
X[,1]<-alpha + X[,2]*(1-alpha)
svals_new <- svd(X)$d
cond_new <- max(svals_new)/min(svals_new)
print(c("The condition matrix when two columns are colinear is: ",cond_new))
```
So, this is now crazy high.
```{r}
errors <- rep(0,N)
for(k in 1:N){
  y <- X %*% beta + rnorm(n)
  betahat <- casl_ols_svd(X,y)
  errors[k] <- sqrt(sum((betahat-beta)^2))
}
avg_error_new <- mean(errors)
print(c("The mean squared error for when two columns are colinear is: ",avg_error_new))
```

Now, to show that it works in ridge regression:
```{r}
beta <- t(bis557::ridge(X,y, lambdas = 0))
errors[k] <- sqrt(sum((betahat-beta)^2))
avg_error_rr <- mean(errors)
print(c("the mean squared error using ridge regression is: ",avg_error_rr))
```
This shows how ridge regression reduces the MSE of a set with numerical instability

***  
##### Question 5: LASSO Penalty
>Consider the LASSO penalty
$$
\frac{1}{2n} ||Y − X \beta||_2^2 + \lambda||\beta||_1.
$$
Show that if $|X^T_j * Y | ≤ n * \lambda$, then $\beta^{LASSO}$ must be zero.  

